---
title: "Gibbs sampling investigation for integrated spatial occupancy models"
description: |
  An introduction to Gibbs sampling for latent Gaussian process models
author:
  - name: Christian Stratton
    affiliation: Montana State University
    affiliation_url: https://math.montana.edu/
  - name: Kathryn Irvine
    affiliation: U.S. Geological Survey
    affiliation_url: https://www.usgs.gov/centers/norock
  - name: Katharine Banner
    affiliation: Montana State University
    affiliation_url: https://math.montana.edu/
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 2
---

```{r setup, include = F}
rm(list = ls())

library(knitr)
hook_chunk <- knitr::knit_hooks$get('chunk')
knit_hooks$set(chunk = function(x, options) {

  # add latex commands if chunk option singlespacing is TRUE
  if(isTRUE(options$singlespacing)){
    return(sprintf("\\singlespacing\n %s \n\\doublespacing", hook_chunk(x, options)))
  } else{
    return(hook_chunk(x, options))
  }
})
knitr::opts_chunk$set(
  fig.align = "center",
  tidy = T,
  singlespacing = TRUE,
  cache = FALSE,
  fig.dim = c(10,8),
  message = FALSE,
  warning = FALSE,
  comment = NA,
  echo = F
)


# packages
packs <- c("dplyr", "nimble", "htmltools", "ggplot2", "sf", "Rcpp", "RcppArmadillo", "inline", "mvtnorm", "readr", "parallel", "xtable", "rstan", "coda", "vegan", "tidyr", "gganimate", "stringr", "scatterplot3d", "plot3D", "plotly", "tidyverse", "ggalluvial", "lubridate", "ggnewscale")
sapply(packs, require, character.only = T)
rm(packs)
options(tidyverse.quiet = TRUE)

# convenience
`%notin%` <- Negate("%in%")

# stan settings
options(mc.cores = parallel::detectCores() - 1)
rstan_options(auto_write = TRUE)

# helper functions
source("helpers.R")
```

# Introduction

The purpose of this document is to develop a framework that simultaneously models a spatial disease process with acoustic count occupancy data. Additionally, we develop a Gibbs sampler for that model. In the sections that follow, we build toward this framework and provide code for Gibbs sampling intermediate models. 

# Univariate normal 

In this section, we provide a Gibbs sampler for a univariate normal sampling model. 

## Sampling model and priors

Sampling model:
\[
y_i \sim N(\mu, \sigma^2)
\]

Priors:
\[
\begin{split}
\mu &\sim N(\mu_0, \tau_0^2) \\
\sigma &\sim IG(a_0, b_0)
\end{split}
\]

## Example simulated data

Data generating values: $\mu = 0$, $\sigma = 2$.

```{r}
sim_norm <- function(n, mu, sigma, seed = NULL){
  if(!is.null(seed)) set.seed(seed)
  y <- rnorm(n, mu, sigma)
  return(y)
}
y <- sim_norm(500, 0, 2, seed = 05172022)
ggplot() + 
  geom_histogram(data = tibble(y = y), aes(x = y)) +
  theme_bw() +
  labs(title = bquote("Simulated data:" ~ mu == "0," ~ sigma == 2))
```

## Full conditional posterior distributions

Derivations are omitted, for now. 

\[
\begin{split}
\mu | y, \sigma^2 &\sim N(m, V) \\
V &= \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}\right)^{-1}\\
m &= V \left(\frac{\mu_0}{\tau_0^2} +  \frac{1}{\sigma^2}\sum_{i=1}^n y_i\right)
\end{split}
\]

\[
\begin{split}
\sigma^2 | y, \mu &\sim \text{Inverse-gamma}(a, b) \\
a &= a_0 + \frac{n}{2} \\
b &= b_0 + \frac{1}{2}\sum_{i=1}^n (y_i - \mu)^2
\end{split}
\]

## Gibbs sampler

```{r, echo = T}
normal_gibbs <- function(num_mcmc, warmup = num_mcmc/2, y, seed = NULL){
  
  if(!is.null(seed)) set.seed(seed)
  
  # convenience
  n <- length(y)
  sum_y <- sum(y)
  
  # storage
  mu_mcmc <- matrix(NA, num_mcmc, 1)
  sigma_mcmc <- matrix(NA, num_mcmc, 1)
  
  # priors
  mu0 <- 0
  tau0 <- 100
  a0 <- .01
  b0 <- .01
  
  # initialize
  mu <- rnorm(1, mu0, tau0); mu_mcmc[1,] <- mu
  sigma <- invgamma::rinvgamma(1, a0, b0); sigma_mcmc[1,] <- sigma
  
  # sampler
  pb <- txtProgressBar(min = 0, max = num_mcmc, style = 3, width = 50, char = "=") 
  for(iter in 2:num_mcmc){
    # mu
    v <- solve(1/tau0^2 + n/sigma^2)
    m <- v * (mu0 / tau0^2 + sum_y / sigma^2)
    mu <- rnorm(1, m, sqrt(v))
    
    # sigma
    a <- a0 + n/2
    b <- b0 + sum((y-mu)^2)/2
    sigma <- sqrt(invgamma::rinvgamma(1, a, b))
    
    # storage
    mu_mcmc[iter,] <- mu
    sigma_mcmc[iter,] <- sigma
    
    # progress
    setTxtProgressBar(pb, iter)
  }
  close(pb)
  
  samples <- cbind(
    mu = mu_mcmc[(warmup+1):num_mcmc,],
    sigma = sigma_mcmc[(warmup+1):num_mcmc,]
  )
  
  return(samples)
  
}
```

### One simulated data set

```{r, echo = T}
# fit model
this_cluster <- makeCluster(3)
samples <- parLapply(
  cl = this_cluster,
  X = 1:3,
  fun = normal_gibbs,
  num_mcmc = 5000,
  warmup = 2500,
  y = y
)
stopCluster(this_cluster)

# summarize
model_summary <- nimble_summary(samples)
model_summary[,c(1, 5, 9:12)]
```

## Synethetic data simulation

```{r, eval = F}
library(parallel)
sim_normal <- function(nsims, n = 400){
    # convenience function
  nimble_summary <- function(fit, warmup = nrow(fit[[1]])/2, thin = 1){
    # convert to coda for normal summary
    fit_warmup <- lapply(fit, function(x) x[(warmup+1):nrow(x),])
    coda_samples <- as.mcmc.list(lapply(fit_warmup, function(x) as.mcmc(
      x, start = warmup+1, end = nrow(fit), thin = thin
    )))
    
    sum <- summary(coda_samples)
    params <- dimnames(sum$statistics)[[1]]
    tmp_sum <- cbind(sum$statistics, sum$quantiles)
    
    # get r hat / n_eff
    mat <- matrix(NA, nrow = nrow(tmp_sum), ncol = 3)
    colnames(mat) <- c("Rhat", "ess_bulk", "ess_tail")
    for(i in 1:nrow(tmp_sum)){
      tmp <- sapply(fit, function(x) x[,i])
      mat[i,] <- c(Rhat(tmp), ess_bulk(tmp), ess_tail(tmp))
    }
    
    # out 
    out <- cbind(tmp_sum, mat)
    return(out)
  }
  
  sum_tbl <- list()
  pb <- txtProgressBar(min = 0, max = nsims, style = 3, width = 50, char = "=")
  for(sim in 1:nsims){
    # simulate data
    dat <- sim_norm(
      n = n, 
      mu = 0,
      sigma = 2, 
      seed = sim
    )
    
    # fit model - parallel
    tmp_sim <- sim
    this_cluster <- makeCluster(3)
    samples <- parLapply(
      cl = this_cluster,
      X = 1:3,
      fun = normal_gibbs,
      num_mcmc = 5000,
      warmup = 2500,
      y = dat
    )
    stopCluster(this_cluster)

    sum <- nimble_summary(samples, warmup = 0)
    sum_tbl[[sim]] <- tibble(
      param = rownames(sum),
      truth = c(0, 2),
      mean = sum[,1],
      lwr = sum[,5],
      upr = sum[,9],
      rhat = sum[,10],
      ess_bulk = sum[,11],
      ess_tail = sum[,12]
    ) %>%
      mutate(sim = tmp_sim)
    
    setTxtProgressBar(pb, sim)
  }
  close(pb)
  
  return(do.call("bind_rows", sum_tbl))
}
normal_sims <- sim_normal(100, 400)
saveRDS(normal_sims, "rds files/normal/normal_sims.rds")
```

```{r}
normal_sims <- readRDS("rds files/normal/normal_sims.rds")
normal_sims %>%
  mutate(
    capture = factor(case_when(
      truth >= lwr & truth <= upr ~ 1, 
      TRUE ~ 0
    ))
  ) %>%
  ggplot() + 
  geom_linerange(
    aes(xmin = lwr, xmax = upr, x = mean, col = capture, y = sim)
  ) +
  geom_vline(
    data = tibble(param = c("mu", "sigma"), int = c(0, 2)),
    aes(xintercept = int),
    linetype = "dotdash"
  ) +
  facet_wrap(~ param, scales = "free_x") +
  theme_bw()
```

# Ordinary least squares regression

In this section, we provide a Gibbs sampler a special case of the multivariate normal distribution - ordinary least squares regression. 

## Sampling model and priors

Sampling model:
\[
y \sim \mathcal{N}(X\beta, \sigma^2 I_n)
\]

Priors:
\[
\begin{split}
\beta &\sim \mathcal{N}(\mu_0, \Sigma_0) \\
\sigma &\sim IG(a_0, b_0)
\end{split}
\]

## Example simulated data

Data generating values: $\beta = c(0,1)$, $\sigma = 1$.

```{r}
sim_reg <- function(n, beta, X, sigma, Omega = diag(n), seed = NULL){
  if(!is.null(seed)) set.seed(seed)
  
  mu <- X %*% beta
  Sigma <- sigma^2 * Omega + diag(.00001, n, n)
  # y <- chol(Sigma) %*% rnorm(n) + mu
  y <- c(mvtnorm::rmvnorm(1, mu, Sigma))
  
  df <- cbind(
    y,
    X[,2:ncol(X), drop=FALSE]
  ) %>%
    as_tibble
  
  names(df)[2:ncol(df)] <- paste0("x", 1:(ncol(X)-1))

  return(
    df
  )
}

n <- 500
dat <- sim_reg(
  n = n, 
  X = cbind(rep(1, n), rnorm(n)),
  beta = c(0, 1),
  sigma = 1, 
  Omega = diag(n),
  seed = 05172022
)
ggplot() + 
  geom_point(data = dat, aes(x = x1, y = y)) +
  theme_bw() +
  labs(title = bquote("Simulated data:" ~ beta == "c(0, 1)," ~ sigma == 1))
```

## Full conditional posterior distributions

Derivations are omitted, for now. 

\[
\begin{split}
\beta | y, \sigma^2 &\sim N(m, V) \\
V &= \left(\frac{1}{\sigma^2}X'X + \Sigma_0^{-1}\right)^{-1}\\
m &= V \left(\frac{1}{\sigma^2}X'y + \Sigma_0^{-1}\mu_0\right)
\end{split}
\]

\[
\begin{split}
\sigma^2 | y, \mu &\sim \text{Inverse-gamma}(a, b) \\
a &= a_0 + \frac{n}{2} \\
b &= b_0 + \frac{1}{2}(y - X\beta)'(y - X\beta)
\end{split}
\]

## Gibbs sampler

```{r, echo = T}
ols_gibbs <- function(num_mcmc, warmup = num_mcmc/2, y, X_, Omega, seed = NULL){
  
  if(!is.null(seed)) set.seed(seed)
  
  # convenience
  n <- nrow(X_)
  p <- ncol(X_)
  
  # storage
  sigma_mcmc <- matrix(NA, num_mcmc, 1)
  beta_mcmc <- matrix(NA, num_mcmc, p)
  
  # priors
  mu0 <- matrix(0, nrow = p, ncol = 1)
  Sigma0 <- 1000 * diag(1, nrow = p, ncol = p)
  Sigma0_inv <- solve(Sigma0)
  prior_prod <- Sigma0_inv %*% mu0
  a0 <- .01
  b0 <- .01
  
  # more convenience
  XtX <- t(X_) %*% X_
  Xty <- t(X_) %*% y
  
  # initialize
  beta <- chol(Sigma0) %*% rnorm(p) + mu0; beta_mcmc[1,] <- beta
  sigma <- invgamma::rinvgamma(1, a0, b0); sigma_mcmc[1,] <- sigma
  sigma2 <- sigma^2
  
  # sampler
  pb <- txtProgressBar(min = 0, max = num_mcmc, style = 3, width = 50, char = "=") 
  for(iter in 2:num_mcmc){
    # beta
    ## assuming Omega is always identity
    V <- solve(1/sigma2 * XtX + Sigma0_inv)
    m <- V %*% (1/sigma2 * Xty + prior_prod)
    
    beta <- chol(V) %*% rnorm(p) + m
    Xb <- X_ %*% beta
    
    # sigma
    a <- a0 + n/2
    b <- c(b0 + .5 * t(y - Xb) %*% (y - Xb))
    sigma <- sqrt(invgamma::rinvgamma(1, a, b))
    sigma2 <- sigma^2
    
    # storage
    beta_mcmc[iter,] <- beta
    sigma_mcmc[iter,] <- sigma
    
    # progress
    setTxtProgressBar(pb, iter)
  }
  close(pb)
  
  samples <- cbind(
    beta = beta_mcmc[(warmup+1):num_mcmc,],
    sigma = sigma_mcmc[(warmup+1):num_mcmc,]
  )
  
  colnames(samples) <- c(paste0('beta', 0:(p-1)), "sigma")
  
  return(samples)
  
}
```

### One simulated data set

```{r, echo = T}
# fit model
this_cluster <- makeCluster(3)
samples <- parLapply(
  cl = this_cluster,
  X = 1:3,
  fun = ols_gibbs,
  num_mcmc = 5000,
  warmup = 2500,
  y = dat$y,
  X_ = cbind(rep(1, 500), dat$x1),
  Omega = diag(500)
)
stopCluster(this_cluster)

# summarize
model_summary <- nimble_summary(samples)
model_summary[,c(1, 5, 9:12)]
```

## Synethetic data simulation

```{r, eval = F}
library(parallel)
sim_ols <- function(nsims, n = 400){
    # convenience function
  nimble_summary <- function(fit, warmup = nrow(fit[[1]])/2, thin = 1){
    # convert to coda for normal summary
    fit_warmup <- lapply(fit, function(x) x[(warmup+1):nrow(x),])
    coda_samples <- as.mcmc.list(lapply(fit_warmup, function(x) as.mcmc(
      x, start = warmup+1, end = nrow(fit), thin = thin
    )))
    
    sum <- summary(coda_samples)
    params <- dimnames(sum$statistics)[[1]]
    tmp_sum <- cbind(sum$statistics, sum$quantiles)
    
    # get r hat / n_eff
    mat <- matrix(NA, nrow = nrow(tmp_sum), ncol = 3)
    colnames(mat) <- c("Rhat", "ess_bulk", "ess_tail")
    for(i in 1:nrow(tmp_sum)){
      tmp <- sapply(fit, function(x) x[,i])
      mat[i,] <- c(Rhat(tmp), ess_bulk(tmp), ess_tail(tmp))
    }
    
    # out 
    out <- cbind(tmp_sum, mat)
    return(out)
  }
  
  sum_tbl <- list()
  pb <- txtProgressBar(min = 0, max = nsims, style = 3, width = 50, char = "=")
  for(sim in 1:nsims){
    # simulate data
    dat <- sim_reg(
      n = n, 
      X = cbind(rep(1, n), rnorm(n)),
      beta = c(0, 1),
      sigma = 1.5, 
      Omega = diag(n),
      seed = sim
    )
    
    # fit model - parallel
    tmp_sim <- sim
    this_cluster <- makeCluster(3)
    samples <- parLapply(
      cl = this_cluster,
      X = 1:3,
      fun = ols_gibbs,
      num_mcmc = 5000,
      warmup = 2500,
      y = dat$y,
      X_ = cbind(rep(1, length(dat$y)), dat$x1),
      Omega = diag(length(dat$y))
    )
    stopCluster(this_cluster)

    sum <- nimble_summary(samples, warmup = 0)
    sum_tbl[[sim]] <- tibble(
      param = rownames(sum),
      truth = c(0, 1, 1.5),
      mean = sum[,1],
      lwr = sum[,5],
      upr = sum[,9],
      rhat = sum[,10],
      ess_bulk = sum[,11],
      ess_tail = sum[,12]
    ) %>%
      mutate(sim = tmp_sim)
    
    setTxtProgressBar(pb, sim)
  }
  close(pb)
  
  return(do.call("bind_rows", sum_tbl))
}
ols_sims <- sim_ols(100, 400)
saveRDS(ols_sims, "rds files/ols/ols_sims.rds")
```

```{r}
ols_sims <- readRDS("rds files/ols/ols_sims.rds")
ols_sims %>%
  mutate(
    capture = factor(case_when(
      truth >= lwr & truth <= upr ~ 1, 
      TRUE ~ 0
    ))
  ) %>%
  ggplot() + 
  geom_linerange(
    aes(xmin = lwr, xmax = upr, x = mean, col = capture, y = sim)
  ) +
  geom_vline(
    data = tibble(param = c("beta0", "beta1", "sigma"), int = c(0, 1, 1.5)),
    aes(xintercept = int),
    linetype = "dotdash"
  ) +
  facet_wrap(~ param, scales = "free_x") +
  theme_bw()
```

