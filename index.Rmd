---
title: "Gibbs sampling investigation for integrated spatial occupancy models"
description: |
  An introduction to Gibbs sampling for latent Gaussian process models
author:
  - name: Christian Stratton
    affiliation: Montana State University
    affiliation_url: https://math.montana.edu/
  - name: Kathryn Irvine
    affiliation: U.S. Geological Survey
    affiliation_url: https://www.usgs.gov/centers/norock
  - name: Katharine Banner
    affiliation: Montana State University
    affiliation_url: https://math.montana.edu/
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 2
bibliography: references.bib
csl: biometrics_notes.csl
---

```{r setup, include = F}
rm(list = ls())

library(knitr)
hook_chunk <- knitr::knit_hooks$get('chunk')
knit_hooks$set(chunk = function(x, options) {

  # add latex commands if chunk option singlespacing is TRUE
  if(isTRUE(options$singlespacing)){
    return(sprintf("\\singlespacing\n %s \n\\doublespacing", hook_chunk(x, options)))
  } else{
    return(hook_chunk(x, options))
  }
})
knitr::opts_chunk$set(
  fig.align = "center",
  tidy = T,
  singlespacing = TRUE,
  cache = FALSE,
  fig.dim = c(10,8),
  message = FALSE,
  warning = FALSE,
  comment = NA,
  echo = F
)


# packages
packs <- c("dplyr", "nimble", "htmltools", "ggplot2", "sf", "Rcpp", "RcppArmadillo", "inline", "mvtnorm", "readr", "parallel", "xtable", "rstan", "coda", "vegan", "tidyr", "gganimate", "stringr", "scatterplot3d", "plot3D", "plotly", "tidyverse", "ggalluvial", "lubridate", "ggnewscale")
sapply(packs, require, character.only = T)
rm(packs)
options(tidyverse.quiet = TRUE)

# convenience
`%notin%` <- Negate("%in%")

# stan settings
options(mc.cores = parallel::detectCores() - 1)
rstan_options(auto_write = TRUE)

# helper functions
source("helpers.R")
```

# Introduction

The purpose of this document is to develop a framework that simultaneously models a spatial disease process with acoustic count occupancy data. Additionally, we develop a Gibbs sampler for that model. In the sections that follow, we build toward this framework and provide code for Gibbs sampling intermediate models. 

# Univariate normal 

In this section, we provide a Gibbs sampler for a univariate normal sampling model. 

## Sampling model and priors

Sampling model:
\[
y_i \sim N(\mu, \sigma^2)
\]

Priors:
\[
\begin{split}
\mu &\sim N(\mu_0, \tau_0^2) \\
\sigma &\sim IG(a_0, b_0)
\end{split}
\]

## Example simulated data

Data generating values: $\mu = 0$, $\sigma = 2$.

```{r}
sim_norm <- function(n, mu, sigma, seed = NULL){
  if(!is.null(seed)) set.seed(seed)
  y <- rnorm(n, mu, sigma)
  return(y)
}
y <- sim_norm(500, 0, 2, seed = 05172022)
ggplot() + 
  geom_histogram(data = tibble(y = y), aes(x = y)) +
  theme_bw() +
  labs(title = bquote("Simulated data:" ~ mu == "0," ~ sigma == 2))
```

## Full conditional posterior distributions

Derivations are omitted, for now. 

\[
\begin{split}
\mu | y, \sigma^2 &\sim N(m, V) \\
V &= \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}\right)^{-1}\\
m &= V \left(\frac{\mu_0}{\tau_0^2} +  \frac{1}{\sigma^2}\sum_{i=1}^n y_i\right)
\end{split}
\]

\[
\begin{split}
\sigma^2 | y, \mu &\sim \text{Inverse-gamma}(a, b) \\
a &= a_0 + \frac{n}{2} \\
b &= b_0 + \frac{1}{2}\sum_{i=1}^n (y_i - \mu)^2
\end{split}
\]

## Gibbs sampler

```{r, echo = T}
normal_gibbs <- function(num_mcmc, warmup = num_mcmc/2, y, seed = NULL){
  
  if(!is.null(seed)) set.seed(seed)
  
  # convenience
  n <- length(y)
  sum_y <- sum(y)
  
  # storage
  mu_mcmc <- matrix(NA, num_mcmc, 1)
  sigma_mcmc <- matrix(NA, num_mcmc, 1)
  
  # priors
  mu0 <- 0
  tau0 <- 100
  a0 <- .01
  b0 <- .01
  
  # initialize
  mu <- rnorm(1, mu0, tau0); mu_mcmc[1,] <- mu
  sigma <- invgamma::rinvgamma(1, a0, b0); sigma_mcmc[1,] <- sigma
  
  # sampler
  pb <- txtProgressBar(min = 0, max = num_mcmc, style = 3, width = 50, char = "=") 
  for(iter in 2:num_mcmc){
    # mu
    v <- solve(1/tau0^2 + n/sigma^2)
    m <- v * (mu0 / tau0^2 + sum_y / sigma^2)
    mu <- rnorm(1, m, sqrt(v))
    
    # sigma
    a <- a0 + n/2
    b <- b0 + sum((y-mu)^2)/2
    sigma <- sqrt(invgamma::rinvgamma(1, a, b))
    
    # storage
    mu_mcmc[iter,] <- mu
    sigma_mcmc[iter,] <- sigma
    
    # progress
    setTxtProgressBar(pb, iter)
  }
  close(pb)
  
  samples <- cbind(
    mu = mu_mcmc[(warmup+1):num_mcmc,],
    sigma = sigma_mcmc[(warmup+1):num_mcmc,]
  )
  
  return(samples)
  
}
```

### One simulated data set

```{r, echo = T}
# fit model
this_cluster <- makeCluster(3)
samples <- parLapply(
  cl = this_cluster,
  X = 1:3,
  fun = normal_gibbs,
  num_mcmc = 5000,
  warmup = 2500,
  y = y
)
stopCluster(this_cluster)

# summarize
model_summary <- nimble_summary(samples)
model_summary[,c(1, 5, 9:12)]
```

## Synthetic data simulation

```{r, eval = F}
library(parallel)
sim_normal <- function(nsims, n = 400){
    # convenience function
  nimble_summary <- function(fit, warmup = nrow(fit[[1]])/2, thin = 1){
    # convert to coda for normal summary
    fit_warmup <- lapply(fit, function(x) x[(warmup+1):nrow(x),])
    coda_samples <- as.mcmc.list(lapply(fit_warmup, function(x) as.mcmc(
      x, start = warmup+1, end = nrow(fit), thin = thin
    )))
    
    sum <- summary(coda_samples)
    params <- dimnames(sum$statistics)[[1]]
    tmp_sum <- cbind(sum$statistics, sum$quantiles)
    
    # get r hat / n_eff
    mat <- matrix(NA, nrow = nrow(tmp_sum), ncol = 3)
    colnames(mat) <- c("Rhat", "ess_bulk", "ess_tail")
    for(i in 1:nrow(tmp_sum)){
      tmp <- sapply(fit, function(x) x[,i])
      mat[i,] <- c(Rhat(tmp), ess_bulk(tmp), ess_tail(tmp))
    }
    
    # out 
    out <- cbind(tmp_sum, mat)
    return(out)
  }
  
  sum_tbl <- list()
  pb <- txtProgressBar(min = 0, max = nsims, style = 3, width = 50, char = "=")
  for(sim in 1:nsims){
    # simulate data
    dat <- sim_norm(
      n = n, 
      mu = 0,
      sigma = 2, 
      seed = sim
    )
    
    # fit model - parallel
    tmp_sim <- sim
    this_cluster <- makeCluster(3)
    samples <- parLapply(
      cl = this_cluster,
      X = 1:3,
      fun = normal_gibbs,
      num_mcmc = 5000,
      warmup = 2500,
      y = dat
    )
    stopCluster(this_cluster)

    sum <- nimble_summary(samples, warmup = 0)
    sum_tbl[[sim]] <- tibble(
      param = rownames(sum),
      truth = c(0, 2),
      mean = sum[,1],
      lwr = sum[,5],
      upr = sum[,9],
      rhat = sum[,10],
      ess_bulk = sum[,11],
      ess_tail = sum[,12]
    ) %>%
      mutate(sim = tmp_sim)
    
    setTxtProgressBar(pb, sim)
  }
  close(pb)
  
  return(do.call("bind_rows", sum_tbl))
}
normal_sims <- sim_normal(100, 400)
saveRDS(normal_sims, "rds files/normal/normal_sims.rds")
```

```{r}
normal_sims <- readRDS("rds files/normal/normal_sims.rds")
normal_sims %>%
  mutate(
    capture = factor(case_when(
      truth >= lwr & truth <= upr ~ 1, 
      TRUE ~ 0
    ))
  ) %>%
  ggplot() + 
  geom_linerange(
    aes(xmin = lwr, xmax = upr, x = mean, col = capture, y = sim)
  ) +
  geom_vline(
    data = tibble(param = c("mu", "sigma"), int = c(0, 2)),
    aes(xintercept = int),
    linetype = "dotdash"
  ) +
  facet_wrap(~ param, scales = "free_x") +
  theme_bw()
```

# Ordinary least squares regression

In this section, we provide a Gibbs sampler a special case of the multivariate normal distribution - ordinary least squares regression. 

## Sampling model and priors

Sampling model:
\[
y \sim \mathcal{N}(X\beta, \sigma^2 I_n)
\]

Priors:
\[
\begin{split}
\beta &\sim \mathcal{N}(\mu_0, \Sigma_0) \\
\sigma &\sim IG(a_0, b_0)
\end{split}
\]

## Example simulated data

Data generating values: $\beta = c(0,1)$, $\sigma = 1$.

```{r}
sim_reg <- function(n, beta, X, sigma, Omega = diag(n), seed = NULL){
  if(!is.null(seed)) set.seed(seed)
  
  mu <- X %*% beta
  Sigma <- sigma^2 * Omega + diag(.00001, n, n)
  # y <- chol(Sigma) %*% rnorm(n) + mu
  y <- c(mvtnorm::rmvnorm(1, mu, Sigma))
  
  df <- cbind(
    y,
    X[,2:ncol(X), drop=FALSE]
  ) %>%
    as_tibble
  
  names(df)[2:ncol(df)] <- paste0("x", 1:(ncol(X)-1))

  return(
    df
  )
}

n <- 500
dat <- sim_reg(
  n = n, 
  X = cbind(rep(1, n), rnorm(n)),
  beta = c(0, 1),
  sigma = 1, 
  Omega = diag(n),
  seed = 05172022
)
ggplot() + 
  geom_point(data = dat, aes(x = x1, y = y)) +
  theme_bw() +
  labs(title = bquote("Simulated data:" ~ beta == "c(0, 1)," ~ sigma == 1))
```

## Full conditional posterior distributions

Derivations are omitted, for now. 

\[
\begin{split}
\beta | y, \sigma^2 &\sim N(m, V) \\
V &= \left(\frac{1}{\sigma^2}X'X + \Sigma_0^{-1}\right)^{-1}\\
m &= V \left(\frac{1}{\sigma^2}X'y + \Sigma_0^{-1}\mu_0\right)
\end{split}
\]

\[
\begin{split}
\sigma^2 | y, \beta &\sim \text{Inverse-gamma}(a, b) \\
a &= a_0 + \frac{n}{2} \\
b &= b_0 + \frac{1}{2}(y - X\beta)'(y - X\beta)
\end{split}
\]

## Gibbs sampler

```{r, echo = T}
ols_gibbs <- function(num_mcmc, warmup = num_mcmc/2, y, X_, Omega, seed = NULL){
  
  if(!is.null(seed)) set.seed(seed)
  
  # convenience
  n <- nrow(X_)
  p <- ncol(X_)
  
  # storage
  sigma_mcmc <- matrix(NA, num_mcmc, 1)
  beta_mcmc <- matrix(NA, num_mcmc, p)
  
  # priors
  mu0 <- matrix(0, nrow = p, ncol = 1)
  Sigma0 <- 1000 * diag(1, nrow = p, ncol = p)
  Sigma0_inv <- solve(Sigma0)
  prior_prod <- Sigma0_inv %*% mu0
  a0 <- .01
  b0 <- .01
  
  # more convenience
  XtX <- t(X_) %*% X_
  Xty <- t(X_) %*% y
  
  # initialize
  beta <- chol(Sigma0) %*% rnorm(p) + mu0; beta_mcmc[1,] <- beta
  sigma <- invgamma::rinvgamma(1, a0, b0); sigma_mcmc[1,] <- sigma
  sigma2 <- sigma^2
  
  # sampler
  pb <- txtProgressBar(min = 0, max = num_mcmc, style = 3, width = 50, char = "=") 
  for(iter in 2:num_mcmc){
    # beta
    ## assuming Omega is always identity
    V <- solve(1/sigma2 * XtX + Sigma0_inv)
    m <- V %*% (1/sigma2 * Xty + prior_prod)
    
    beta <- chol(V) %*% rnorm(p) + m
    Xb <- X_ %*% beta
    
    # sigma
    a <- a0 + n/2
    b <- c(b0 + .5 * t(y - Xb) %*% (y - Xb))
    sigma <- sqrt(invgamma::rinvgamma(1, a, b))
    sigma2 <- sigma^2
    
    # storage
    beta_mcmc[iter,] <- beta
    sigma_mcmc[iter,] <- sigma
    
    # progress
    setTxtProgressBar(pb, iter)
  }
  close(pb)
  
  samples <- cbind(
    beta = beta_mcmc[(warmup+1):num_mcmc,],
    sigma = sigma_mcmc[(warmup+1):num_mcmc,]
  )
  
  colnames(samples) <- c(paste0('beta', 0:(p-1)), "sigma")
  
  return(samples)
  
}
```

### One simulated data set

```{r, echo = T}
# fit model
this_cluster <- makeCluster(3)
samples <- parLapply(
  cl = this_cluster,
  X = 1:3,
  fun = ols_gibbs,
  num_mcmc = 5000,
  warmup = 2500,
  y = dat$y,
  X_ = cbind(rep(1, 500), dat$x1),
  Omega = diag(500)
)
stopCluster(this_cluster)

# summarize
model_summary <- nimble_summary(samples)
model_summary[,c(1, 5, 9:12)]
```

## Synthetic data simulation

```{r, eval = F}
library(parallel)
sim_ols <- function(nsims, n = 400){
    # convenience function
  nimble_summary <- function(fit, warmup = nrow(fit[[1]])/2, thin = 1){
    # convert to coda for normal summary
    fit_warmup <- lapply(fit, function(x) x[(warmup+1):nrow(x),])
    coda_samples <- as.mcmc.list(lapply(fit_warmup, function(x) as.mcmc(
      x, start = warmup+1, end = nrow(fit), thin = thin
    )))
    
    sum <- summary(coda_samples)
    params <- dimnames(sum$statistics)[[1]]
    tmp_sum <- cbind(sum$statistics, sum$quantiles)
    
    # get r hat / n_eff
    mat <- matrix(NA, nrow = nrow(tmp_sum), ncol = 3)
    colnames(mat) <- c("Rhat", "ess_bulk", "ess_tail")
    for(i in 1:nrow(tmp_sum)){
      tmp <- sapply(fit, function(x) x[,i])
      mat[i,] <- c(Rhat(tmp), ess_bulk(tmp), ess_tail(tmp))
    }
    
    # out 
    out <- cbind(tmp_sum, mat)
    return(out)
  }
  
  sum_tbl <- list()
  pb <- txtProgressBar(min = 0, max = nsims, style = 3, width = 50, char = "=")
  for(sim in 1:nsims){
    # simulate data
    dat <- sim_reg(
      n = n, 
      X = cbind(rep(1, n), rnorm(n)),
      beta = c(0, 1),
      sigma = 1.5, 
      Omega = diag(n),
      seed = sim
    )
    
    # fit model - parallel
    tmp_sim <- sim
    this_cluster <- makeCluster(3)
    samples <- parLapply(
      cl = this_cluster,
      X = 1:3,
      fun = ols_gibbs,
      num_mcmc = 5000,
      warmup = 2500,
      y = dat$y,
      X_ = cbind(rep(1, length(dat$y)), dat$x1),
      Omega = diag(length(dat$y))
    )
    stopCluster(this_cluster)

    sum <- nimble_summary(samples, warmup = 0)
    sum_tbl[[sim]] <- tibble(
      param = rownames(sum),
      truth = c(0, 1, 1.5),
      mean = sum[,1],
      lwr = sum[,5],
      upr = sum[,9],
      rhat = sum[,10],
      ess_bulk = sum[,11],
      ess_tail = sum[,12]
    ) %>%
      mutate(sim = tmp_sim)
    
    setTxtProgressBar(pb, sim)
  }
  close(pb)
  
  return(do.call("bind_rows", sum_tbl))
}
ols_sims <- sim_ols(100, 400)
saveRDS(ols_sims, "rds files/ols/ols_sims.rds")
```

```{r}
ols_sims <- readRDS("rds files/ols/ols_sims.rds")
ols_sims %>%
  mutate(
    capture = factor(case_when(
      truth >= lwr & truth <= upr ~ 1, 
      TRUE ~ 0
    ))
  ) %>%
  ggplot() + 
  geom_linerange(
    aes(xmin = lwr, xmax = upr, x = mean, col = capture, y = sim)
  ) +
  geom_vline(
    data = tibble(param = c("beta0", "beta1", "sigma"), int = c(0, 1, 1.5)),
    aes(xintercept = int),
    linetype = "dotdash"
  ) +
  facet_wrap(~ param, scales = "free_x") +
  theme_bw()
```

# Generalized least squares regression

In this section, we provide a Gibbs sampler a special case of the multivariate normal distribution - generalized least squares regression. 

## Sampling model and priors

Sampling model:
\[
\begin{split}
y &\sim \mathcal{N}(X\beta, \Sigma) \\
\Sigma &= \sigma^2\Omega, \Omega\text{ known}
\end{split}
\]

Priors:
\[
\begin{split}
\beta &\sim \mathcal{N}(\mu_0, \Sigma_0) \\
\sigma &\sim IG(a_0, b_0)
\end{split}
\]

## Example simulated data

Data generating values: $\beta = c(0,1)$, $\sigma = 1$, $\Omega =$ exp(-as.matrix(dist(1:n)^2/3)).

```{r}
sim_reg <- function(n, beta, X, sigma, Omega = diag(n), seed = NULL){
  if(!is.null(seed)) set.seed(seed)
  
  mu <- X %*% beta
  Sigma <- sigma^2 * Omega + diag(.00001, n, n)
  # y <- chol(Sigma) %*% rnorm(n) + mu
  y <- c(mvtnorm::rmvnorm(1, mu, Sigma))
  
  df <- cbind(
    y,
    X[,2:ncol(X), drop=FALSE]
  ) %>%
    as_tibble
  
  names(df)[2:ncol(df)] <- paste0("x", 1:(ncol(X)-1))

  return(
    df
  )
}

n <- 500
dat <- sim_reg(
  n = n, 
  X = cbind(rep(1, n), rnorm(n)),
  beta = c(0, 1),
  sigma = 1, 
  Omega = exp(-as.matrix(dist(1:n)^2/3)),
  seed = 05172022
)
ggplot() + 
  geom_point(data = dat, aes(x = x1, y = y)) +
  theme_bw() +
  labs(title = bquote("Simulated data:" ~ beta == "c(0, 1)," ~ sigma == "1," ~ Omega == "exp(-as.matrix(dist(1:n)^2/3))"))
```

## Full conditional posterior distributions

Derivations are omitted, for now. 

\[
\begin{split}
\beta | y, \sigma^2 &\sim N(m, V) \\
V &= \left(\frac{1}{\sigma^2}X' \Omega^{-1}  X + \Sigma_0^{-1}\right)^{-1}\\
m &= V \left(\frac{1}{\sigma^2}X'\Omega^{-1}y + \Sigma_0^{-1}\mu_0\right)
\end{split}
\]

\[
\begin{split}
\sigma^2 | y, \beta &\sim \text{Inverse-gamma}(a, b) \\
a &= a_0 + \frac{n}{2} \\
b &= b_0 + \frac{1}{2}(y - X\beta)' \Omega^{-1}(y - X\beta)
\end{split}
\]

## Gibbs sampler

```{r, echo = T}
gls_gibbs <- function(seed, num_mcmc, warmup = num_mcmc/2, y, X_, Omega){
  # hoff text pg 189 for reassurance
  
  # seed
  set.seed(seed)
  
  # convenience
  n <- nrow(X_)
  p <- ncol(X_)
  
  # storage
  sigma_mcmc <- matrix(NA, num_mcmc, 1)
  beta_mcmc <- matrix(NA, num_mcmc, p)
  
  # priors
  mu0 <- matrix(0, nrow = p, ncol = 1)
  Sigma0 <- 1000 * diag(1, p, p)
  Sigma0_inv <- solve(Sigma0)
  prior_prod <- Sigma0_inv %*% mu0
  a0 <- .1
  b0 <- .1
  
  # initialize
  beta <- chol(Sigma0) %*% rnorm(p) + mu0; beta_mcmc[1,] <- beta
  # beta <- mvtnorm::rmvnorm(1, mu0, Sigma0); beta_mcmc[1,] <- beta
  sigma <- invgamma::rinvgamma(1, a0, b0); sigma_mcmc[1,] <- sigma
  sigma2 <- sigma^2
  Omega_inv <- solve(Omega + diag(.00001, n, n))
  
  # more convenience
  XtX <- t(X_) %*% X_
  XtOX <- t(X_) %*% Omega_inv %*% X_
  Xty <- t(X_) %*% y
  XtOy <- t(X_) %*% Omega_inv %*% y
  
  # sampler
  pb <- txtProgressBar(min = 0, max = num_mcmc, style = 3, width = 50, char = "=")
  for(iter in 2:num_mcmc){
    # beta
    V <- solve(1/sigma2 * XtOX + Sigma0_inv)
    m <- V %*% (1/sigma2 * XtOy + prior_prod)
    
    beta <- chol(V) %*% rnorm(p) + m
    # beta <- matrix(c(mvtnorm::rmvnorm(1, m, V)), ncol = 1)
    Xb <- X_ %*% beta
    
    # sigma
    a <- a0 + n/2
    # a <- (a0 + n)/2
    b <- c(b0 + .5 * t(y - Xb) %*% Omega_inv %*% (y - Xb))
    # b <- (a0*b0 + t(y - Xb) %*% Omega_inv %*% (y - Xb))/2
    sigma <- sqrt(invgamma::rinvgamma(1, a, b))
    sigma2 <- sigma^2
    
    # storage
    beta_mcmc[iter,] <- beta
    sigma_mcmc[iter,] <- sigma
    
    # progress
    setTxtProgressBar(pb, iter)
  }
  close(pb)
  
  samples <- cbind(
    beta_mcmc[(warmup+1):num_mcmc,],
    sigma_mcmc[(warmup+1):num_mcmc,]
  )
  
  colnames(samples) <- c(paste0(rep("beta[", p), 1:p, rep("]", p)), "sigma")
  
  return(samples)
  
}
```

### One simulated data set

```{r, echo = T}
# fit model
this_cluster <- makeCluster(3)
samples <- parLapply(
  cl = this_cluster,
  X = 1:3,
  fun = gls_gibbs,
  num_mcmc = 5000,
  warmup = 2500,
  y = dat$y,
  X_ = cbind(rep(1, 500), dat$x1),
  Omega = exp(-as.matrix(dist(1:n)^2/3))
)
stopCluster(this_cluster)

# summarize
model_summary <- nimble_summary(samples)
model_summary[,c(1, 5, 9:12)]
```

## Synthetic data simulation

```{r, eval = F}
library(parallel)
sim_gls <- function(nsims, n = 400){
    # convenience function
  nimble_summary <- function(fit, warmup = nrow(fit[[1]])/2, thin = 1){
    # convert to coda for normal summary
    fit_warmup <- lapply(fit, function(x) x[(warmup+1):nrow(x),])
    coda_samples <- as.mcmc.list(lapply(fit_warmup, function(x) as.mcmc(
      x, start = warmup+1, end = nrow(fit), thin = thin
    )))
    
    sum <- summary(coda_samples)
    params <- dimnames(sum$statistics)[[1]]
    tmp_sum <- cbind(sum$statistics, sum$quantiles)
    
    # get r hat / n_eff
    mat <- matrix(NA, nrow = nrow(tmp_sum), ncol = 3)
    colnames(mat) <- c("Rhat", "ess_bulk", "ess_tail")
    for(i in 1:nrow(tmp_sum)){
      tmp <- sapply(fit, function(x) x[,i])
      mat[i,] <- c(Rhat(tmp), ess_bulk(tmp), ess_tail(tmp))
    }
    
    # out 
    out <- cbind(tmp_sum, mat)
    return(out)
  }
  
  sum_tbl <- list()
  pb <- txtProgressBar(min = 0, max = nsims, style = 3, width = 50, char = "=")
  for(sim in 1:nsims){
    # simulate data
    dat <- sim_reg(
      n = n, 
      X = cbind(rep(1, n), rnorm(n)),
      beta = c(0, 1),
      sigma = 1.5, 
      Omega = exp(-as.matrix(dist(1:n)^2/3)),
      seed = sim
    )
    
    # fit model - parallel
    tmp_sim <- sim
    this_cluster <- makeCluster(3)
    samples <- parLapply(
      cl = this_cluster,
      X = 1:3,
      fun = gls_gibbs,
      num_mcmc = 5000,
      warmup = 2500,
      y = dat$y,
      X_ = cbind(rep(1, length(dat$y)), dat$x1),
      Omega = exp(-as.matrix(dist(1:length(dat$y))^2/3))
    )
    stopCluster(this_cluster)

    sum <- nimble_summary(samples, warmup = 0)
    sum_tbl[[sim]] <- tibble(
      param = rownames(sum),
      truth = c(0, 1, 1.5),
      mean = sum[,1],
      lwr = sum[,5],
      upr = sum[,9],
      rhat = sum[,10],
      ess_bulk = sum[,11],
      ess_tail = sum[,12]
    ) %>%
      mutate(sim = tmp_sim)
    
    setTxtProgressBar(pb, sim)
  }
  close(pb)
  
  return(do.call("bind_rows", sum_tbl))
}
gls_sims <- sim_gls(100, 400)
saveRDS(gls_sims, "rds files/gls/gls_sims.rds")
```

```{r}
gls_sims <- readRDS("rds files/gls/gls_sims.rds")
gls_sims %>%
  mutate(
    capture = factor(case_when(
      truth >= lwr & truth <= upr ~ 1, 
      TRUE ~ 0
    ))
  ) %>%
  ggplot() + 
  geom_linerange(
    aes(xmin = lwr, xmax = upr, x = mean, col = capture, y = sim)
  ) +
  geom_vline(
    data = tibble(param = c("beta[1]", "beta[2]", "sigma"), int = c(0, 1, 1.5)),
    aes(xintercept = int),
    linetype = "dotdash"
  ) +
  facet_wrap(~ param, scales = "free_x") +
  theme_bw()
```

# Gaussian process

In this section, we provide a Gibbs sampler for a Gaussian process (GP). We include a Metropolis-Hastings sampler on the length-scale parameter in the GP, with an adaptive proposal distribution.

## Sampling model and priors

Sampling model:
\[
\begin{split}
y &\sim \mathcal{N}(X\beta, \Sigma) \\
\Sigma &= \sigma^2 \Omega = \sigma^2\exp\left(-\frac{d_{ij}^2}{2\phi^2}\right)
\end{split}
\]
where $d_{ij}$ represents the distance between two sample locations. 

Priors:
\[
\begin{split}
\beta &\sim \mathcal{N}(\mu_0, \Sigma_0) \\
\sigma &\sim IG(a_0, b_0) \\
\phi &\sim IG(c_0, d_0)
\end{split}
\]

Note that $\{\sigma^2, \phi\}$ are not individually identifiable, but are jointly identifiable. To resolve this issue, a mildly informative prior can be placed on the length-scale parameter. Michael Betancourt, a developer of Stan, provides a nice write-up of the problem and Stan code to estimate an informative IG prior for the length-scale parameter. This prior places low prior probability that the length-scale is less than some minimum value or exceeds some maximum value. For more detail, see [this write-up](https://betanalpha.github.io/assets/case_studies/gaussian_processes.html#323_Informative_Prior_Model).

These values can be chosen to be the minimum and maximum observed distances. As a result, we are loosely assuming that near-independence is realized with the observed spatial domain. 

## Example simulated data

Data generating values: $\beta = 0$, $\sigma = 1$, $\phi = 3$.

```{r}
sim_gp <- function(n = 100, seed = 1, sigma = 1, phi = 1, X = matrix(1, nrow = n), beta = 0, delta = 1e-6){
  # function to simulated occupancy on a square grid
  
  # useful functions
  distance <- function(x){
    dist.vec <- parallelDist::parDist(x)
    dist <- as.matrix(dist.vec)
    return(dist)
  }

  # housekeeping
  if((sqrt(n) %% 1) != 0) stop("n should be a perfect square")
  set.seed(seed)
  
  # create grid
  sfc <- st_sfc(st_polygon(list(rbind(c(0,0), c(sqrt(n),0), c(sqrt(n),sqrt(n)), c(0,0)))))
  grid <- st_as_sf(st_make_grid(sfc, cellsize = 1, square = TRUE)) %>% as_tibble
  names(grid) <- "geometry"
  rm(sfc)
  
  # spatial random effects
  coords <- grid %>%
    st_as_sf %>%
    st_centroid %>%
    st_coordinates
  
  dist_mat <- coords %>%
    as.matrix %>%
    distance
  
  # dist_mat <- dist_mat / max(dist_mat) # normalize max dist to 1
  
  Sigma <- sigma^2 * exp(-dist_mat^2 / (2*phi^2)) + diag(delta, dim(dist_mat))
  grid$y <- c(mvtnorm::rmvnorm(1, X %*% beta, Sigma))
  # grid$y <- c(chol(Sigma) %*% rnorm(n) + X %*% beta) 

  out <- list(
    df = grid,
    params = list(
      phi = phi, sigma = sigma, phi = phi, beta = beta
    ),
    dist_mat = dist_mat,
    coords = coords
  )
  
  return(out)
}

sim_dat <- sim_gp(
  n = 10^2, 
  sigma = 1,
  phi = 3, 
  seed = 06302022, 
  delta = 1e-6
)
sim_dat$df %>%
  st_as_sf %>% 
  ggplot() +
  geom_sf(aes(fill = y)) +
  labs(
    title = "Simulated response (y)",
    subtitle = bquote(beta == 0 ~ "," ~ sigma == 1 ~ "," ~ phi == 3)
  ) +
  theme_bw()
```

## Full conditional posterior distributions

Derivations are omitted, for now. There are Gibbs draws for $\{\beta, \sigma^2\}$ and a MH draw for $\phi$. To speed up time to convergence, and generally make the model easier to fit, we implement an adaptive MH algorithm. For full details, see @haario2001.

\[
\begin{split}
\beta | y, \sigma^2 &\sim N(m, V) \\
V &= \left(\frac{1}{\sigma^2}X' \Omega^{-1}  X + \Sigma_0^{-1}\right)^{-1}\\
m &= V \left(\frac{1}{\sigma^2}X'\Omega^{-1}y + \Sigma_0^{-1}\mu_0\right)
\end{split}
\]

\[
\begin{split}
\sigma^2 | y, \beta &\sim \text{Inverse-gamma}(a, b) \\
a &= a_0 + \frac{n}{2} \\
b &= b_0 + \frac{1}{2}(y - X\beta)' \Omega^{-1}(y - X\beta)
\end{split}
\]

## Gibbs sampler

```{r, echo = T}
gp_mhgibbs <- function(
    seed, num_mcmc, warmup = num_mcmc/2, y, X_, dist, 
    phi_a = .1, phi_b = .1, sigma_a = .1, sigma_b = .1,
    delta = .0001, 
    initial_prop_var = .01, adapt = TRUE, adapt_period = .1*num_mcmc, sd = 2.4^2, epsilon = 1e-9){
  # hoff text pg 189 for reassurance on posteriors
  # heikki haario (2001) - An adaptive Metropolis algorithm
  
  # seed
  set.seed(seed)
  
  # convenience
  n <- nrow(X_)
  p <- ncol(X_)
  
  # storage
  sigma_mcmc <- matrix(NA, num_mcmc, 1)
  phi_mcmc <- matrix(NA, num_mcmc, 1)
  accept_ratio <- matrix(0, num_mcmc, 1)
  beta_mcmc <- matrix(NA, num_mcmc, p)
  
  # priors
  mu0 <- matrix(0, nrow = p, ncol = 1)
  Sigma0 <- 100 * diag(1, p, p)
  Sigma0_inv <- solve(Sigma0)
  prior_prod <- Sigma0_inv %*% mu0
  a0 <- sigma_a
  b0 <- sigma_b
  
  # initialize
  beta <- chol(Sigma0) %*% rnorm(p) + mu0; beta_mcmc[1,] <- beta
  sigma <- invgamma::rinvgamma(1, a0, b0); sigma_mcmc[1,] <- sigma
  sigma2 <- sigma^2
  phi <- invgamma::rinvgamma(1, phi_a, phi_b); phi_mcmc[1,] <- phi
  phi2 <- phi^2
  Omega <- exp(-dist^2/(2*phi2)) + diag(delta, n, n)
  Omega_inv <- solve(Omega)

  XtOX <- t(X_) %*% Omega_inv %*% X_
  XtOy <- t(X_) %*% Omega_inv %*% y
  
  # adaptive
  Ct <- initial_prop_var
  
  # sampler
  message(paste0("Beginning sampling at "), Sys.time())
  pb <- txtProgressBar(min = 0, max = num_mcmc, style = 3, width = 50, char = "=")
  for(iter in 2:num_mcmc){
    # beta
    V <- solve(1/sigma2 * XtOX + Sigma0_inv) + diag(delta, p, p)
    m <- V %*% (1/sigma2 * XtOy + prior_prod)
    
    beta <- chol(V) %*% rnorm(p) + m
    Xb <- X_ %*% beta
    
    # sigma
    a <- a0 + n/2
    b <- c(b0 + .5 * t(y - Xb) %*% Omega_inv %*% (y - Xb))
    sigma <- sqrt(invgamma::rinvgamma(1, a, b))
    sigma2 <- sigma^2
    
    # phi - metropolis
    ## adaptive
    if(adapt){
      if(iter <= adapt_period){
        Ct <- initial_prop_var
      } else if(iter > adapt_period){
        Ct <- sd * cov(phi_mcmc[1:(iter-1),,drop = F]) + sd * epsilon
      }
    }
    
    ## proposal
    phi_s <- -1
    while(phi_s <= 0) phi_s <- phi + rnorm(1, 0, sqrt(Ct))
    phi2_s <- phi_s^2
    Omega_s <- exp(-dist^2/(2*phi2_s)) + diag(delta, n, n)

    ## evaluate proposal
    log_post_current <- mvtnorm::dmvnorm(y, Xb, sigma2 * Omega, log = T) + 
      invgamma::dinvgamma(phi, phi_a, phi_b, log = T)
    log_post_s <- mvtnorm::dmvnorm(y, Xb, sigma2 * Omega_s, log = T) +
      invgamma::dinvgamma(phi_s, phi_a, phi_b, log = T)
    log_r <- log_post_s - log_post_current
    if(log(runif(1)) < log_r){
      phi <- phi_s
      phi2 <- phi_s^2
      Omega <- exp(-dist^2/(2*phi2)) + diag(delta, n, n)
      Omega_inv <- solve(Omega)
      XtOX <- t(X_) %*% Omega_inv %*% X_
      XtOy <- t(X_) %*% Omega_inv %*% y

      accept_ratio[iter,] <- 1
    }
    
    # storage
    beta_mcmc[iter,] <- beta
    sigma_mcmc[iter,] <- sigma
    phi_mcmc[iter,] <- phi
    
    # progress
    setTxtProgressBar(pb, iter)
  }
  close(pb)
  message(paste0("Ending sampling at "), Sys.time())
  
  samples <- cbind(
    beta_mcmc[(warmup+1):num_mcmc,],
    sigma_mcmc[(warmup+1):num_mcmc,],
    phi_mcmc[(warmup+1):num_mcmc,]
  )
  
  colnames(samples) <- c(paste0(rep("beta[", p), 1:p, rep("]", p)), "sigma", "phi")
  
  return(
    list(
      samples = samples,
      accept_ratio = accept_ratio
    )
  )
  
}
```

### One simulated data set

```{r, echo = T, eval = F}
# priors for the length-scale
## estimate an IG dist that place .01 probability on either tail
# stan(
#   file = 'stan programs/invgamPars.stan',
#   data = list(l=1.5, u=16), # (min and max obs distances)
#   iter = 1,
#   warmup = 0,
#   chains = 1,
#   algorithm = "Fixed_param"
# )

# fit model
this_cluster <- makeCluster(3)
samples <- parLapply(
  cl = this_cluster,
  X = 1:3,
  fun = gp_mhgibbs,
  num_mcmc = 10000,
  warmup = 5000,
  y = sim_dat$df$y, 
  X_ = cbind(
    rep(1, nrow(sim_dat$df))
  ),
  dist = sim_dat$dist_mat,
  phi_a = 4,
  phi_b = 11.2,
  sigma_a = 1,
  sigma_b = 1,
  initial_prop_var = .10^2,
  adapt = TRUE,
  delta = 1e-06
)
stopCluster(this_cluster)
saveRDS(samples, "rds files/gp/gp_onefit.rds")
```

```{r, echo = T}
samples <- readRDS("rds files/gp/gp_onefit.rds")

# summarize
model_summary <- nimble_summary(
  list(
    samples[[1]][[1]], samples[[2]][[1]], samples[[3]][[1]] 
  )
)
model_summary[,c(1, 5, 9:12)]
```

## Synthetic data simulation

```{r, eval = F}
library(parallel)
gp_sim <- function(nsims, n = 100){
  sum_tbl <- list()
  pb <- txtProgressBar(min = 0, max = nsims, style = 3, width = 50, char = "=")
  for(sim in 1:nsims){
    # simulate data
    sim_dat <- sim_gp(
      n = n, 
      sigma = 1, 
      phi = 3, 
      seed = sim,
      delta = 1e-9
    )
    
    # fit model - parallel
    tmp_sim <- sim
    this_cluster <- makeCluster(3)
    fit <- parLapply(
      cl = this_cluster,
      X = 1:3,
      fun = gp_mhgibbs,
      num_mcmc = 15000,
      warmup = 10000,
      y = sim_dat$df$y, 
      X_ = cbind(
        rep(1, nrow(sim_dat$df))
      ),
      dist = sim_dat$dist_mat,
      phi_a = 1,
      phi_b = 1,
      sigma_a = 1,
      sigma_b = 1,
      initial_prop_var = .05^2,
      adapt = FALSE,
      delta = 1e-09
    )
    stopCluster(this_cluster)
    
    sum <- nimble_summary(list(fit[[1]]$samples, fit[[2]]$samples, fit[[3]]$samples), warmup = 0)
    sum_tbl[[sim]] <- tibble(
      param = rownames(sum),
      truth = c(0, 1, 3),
      mean = sum[,1],
      lwr = sum[,5],
      upr = sum[,9],
      rhat = sum[,10],
      ess_bulk = sum[,11],
      ess_tail = sum[,12]
    ) %>%
      mutate(sim = tmp_sim)
    
    setTxtProgressBar(pb, sim)
  }
  close(pb)
  
  return(do.call("bind_rows", sum_tbl))
}
gp_sims <- gp_sim(100, 100)
saveRDS(gp_sims, "rds files/gp/gp_sims_1_3.rds")
```

```{r}
gp_sims <- readRDS("rds files/gp/gp_sims_1_3.rds")
gp_sims %>%
  mutate(
    capture = factor(case_when(
      truth >= lwr & truth <= upr ~ 1, 
      TRUE ~ 0
    ))
  ) %>%
  ggplot() + 
  geom_linerange(
    aes(xmin = lwr, xmax = upr, x = mean, col = capture, y = sim)
  ) +
  geom_vline(
    data = tibble(param = c("beta[1]", "phi", "sigma"), int = c(0, 3, 1)),
    aes(xintercept = int),
    linetype = "dotdash"
  ) +
  facet_wrap(~ param, scales = "free_x") +
  theme_bw()
```

# Gaussian process with prediction

```{r}
rm(list = ls()[-which(ls() == "hook_chunk")])
source("helpers.R")
```

In this section, we extend the previous model to make predictions at unsampled locations by leveraging the properties of multivariate normal distributions. 

## Sampling model and priors

Sampling model:
\[
\begin{split}
y &\sim \mathcal{N}(X\beta, \Sigma) \\
\Sigma &= \sigma^2 \Omega = \sigma^2\exp\left(-\frac{d_{ij}^2}{2\phi^2}\right)
\end{split}
\]
where $d_{ij}$ represents the distance between two sample locations. 

Priors:
\[
\begin{split}
\beta &\sim \mathcal{N}(\mu_0, \Sigma_0) \\
\sigma &\sim IG(a_0, b_0) \\
\phi &\sim IG(c_0, d_0)
\end{split}
\]

## Jointly Gaussian random variables

Let $X_1$ denote sampled locations, $X_2$ denote unsampled locations, $y_1$ denote the observed data at locations $X_1$ and $y_2$ denote the predicted response at locations $X_2$. To make predictions, we seek the posterior predictive distribution $p(y_2 | y_1, X_1, X_2)$. Since $y_1$ and $y_2$ are jointly Gaussian, as they come from the same normal distribution, we can write:
\[
\begin{split}
\begin{bmatrix} y_1 \\ y_2 \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}, \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix} \right)
\end{split}
\]
where $\Sigma_{11} = k(X_1, X_1)$, $\Sigma_{22} = k(X_2, X_2)$, $\Sigma_{12} = k(X_1, X_2) = \Sigma_{21}'$ and $k(x, y) = \sigma^2\exp\left(-\frac{d_{ij}}{2\phi^2}\right)$, where $d_{ij}$ is the distance between locations $x$ and $y$.

The conditional distribution is given by:
\[
\begin{split}
p(y_2 | y_1, X_1, X_2) = \mathcal{N}\left(\mu_{2|1}, \Sigma_{2|1}\right)
\end{split}
\]
where $\mu_{2|1} = \mu_2 + \Sigma_{21}\Sigma_{11}^{-1}(y_1 - \mu_1) = (\Sigma_{11}^{-1}\Sigma_{12})'(y_1 - \mu_1)$ (assuming a prior mean of 0) and $\Sigma_{2|1} = \Sigma_{22} - (\Sigma_{11}^{-1}\Sigma_{12})'\Sigma_{12}$.

## Example simulated data

Data generating values: $\beta = 0$, $\sigma = 1$, $\phi = 3$.

```{r}
sim_gp_predict <- function(n = 100, seed = 1, sigma = 1, phi = 1, mu = 0, delta = 1e-9, obs_z_prop = .5){
  # useful functions
  distance <- function(x){
    dist.vec <- parallelDist::parDist(x)
    dist <- as.matrix(dist.vec)
    return(dist)
  }

  # housekeeping
  if((sqrt(n) %% 1) != 0) stop("n should be a perfect square")
  set.seed(seed)
  
  # create grid
  sfc <- st_sfc(st_polygon(list(rbind(c(0,0), c(sqrt(n),0), c(sqrt(n),sqrt(n)), c(0,0)))))
  grid <- st_as_sf(st_make_grid(sfc, cellsize = 1, square = TRUE)) %>% as_tibble
  names(grid) <- "geometry"
  rm(sfc)
  
  # spatial random effects
  coords <- grid %>%
    st_as_sf %>%
    st_centroid %>%
    st_coordinates

  dist_mat <- coords %>%
    as.matrix %>%
    distance
  
  Sigma <- sigma^2 * exp(-dist_mat^2 / (2*phi^2)) + diag(delta, dim(dist_mat))
  grid$z <- c(mvtnorm::rmvnorm(1, rep(mu, n), Sigma))
  
  grid2 <- grid %>%
    mutate(obs_z_ind = rbinom(n, 1, obs_z_prop))

 grid2 <- grid2 %>%
    mutate(
      obs_z = ifelse(obs_z_ind == 1, z, NA)
    )

  out <- list(
    df = grid2,
    params = list(
      phi = phi, sigma = sigma, phi = phi
    ),
    dist_mat = dist_mat,
    coords = coords
  )
  
  return(out)
}

data <- sim_gp_predict(
  n = 10^2, 
  sigma = 1,
  phi = 3, 
  seed = 06302022, 
  delta = 1e-6,
  obs_z_prop = .6
)

p1 <- data$df %>%
  st_as_sf %>%
  ggplot() +
  geom_sf(aes(fill = z)) +
  theme_bw() +
  labs(title = "Population")

p2 <- data$df %>%
  st_as_sf %>%
  filter(obs_z_ind == 1) %>%
  ggplot() +
  geom_sf(aes(fill = z)) +
  theme_bw() +
  labs(title = "Observed data")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

## Full conditional posterior distributions

Derivations are omitted, for now. There are Gibbs draws for $\{\beta, \sigma^2\}$ and a MH draw for $\phi$. To speed up time to convergence, and generally make the model easier to fit, we implement an adaptive MH algorithm. For full details, see @haario2001.

\[
\begin{split}
\beta | y, \sigma^2 &\sim N(m, V) \\
V &= \left(\frac{1}{\sigma^2}X' \Omega^{-1}  X + \Sigma_0^{-1}\right)^{-1}\\
m &= V \left(\frac{1}{\sigma^2}X'\Omega^{-1}y + \Sigma_0^{-1}\mu_0\right)
\end{split}
\]

\[
\begin{split}
\sigma^2 | y, \beta &\sim \text{Inverse-gamma}(a, b) \\
a &= a_0 + \frac{n}{2} \\
b &= b_0 + \frac{1}{2}(y - X\beta)' \Omega^{-1}(y - X\beta)
\end{split}
\]

## Gibbs sampler

```{r, echo = T}
data_prep <- function(dat, sp_ind = "obs_z_ind", sp = "obs_z"){
  # useful functions
  distance <- function(x){
    dist.vec <- parallelDist::parDist(x)
    dist <- as.matrix(dist.vec)
    return(dist)
  }
  
  # storage
  out <- list()
  out$data <- dat
  dat$ndx <- 1:nrow(dat)
  
  # grab response data and spatial data
  out$sp <- unlist(unname(dat[which(dat[,sp_ind] == 1),sp]))
  out$sp_row_num <- unlist(unname(dat[which(dat[,sp_ind] == 1),"ndx"]))

  # observed distance matrix
  out$dist11 <- dat[which(dat[,sp_ind] == 1),] %>%
    st_as_sf %>%
    st_centroid %>%
    st_coordinates %>%
    as.matrix %>%
    distance
  
  # prediction distance matrix
  out$dist22 <- dat[which(dat[,sp_ind] == 0),] %>%
    st_as_sf %>%
    st_centroid %>%
    st_coordinates %>%
    as.matrix %>%
    distance
  
  dist12 <- bind_rows(
    dat[which(dat[,sp_ind] == 1),],
    dat[which(dat[,sp_ind] == 0),]
  ) %>%
    st_as_sf %>%
    st_centroid %>%
    st_coordinates %>%
    as.matrix %>%
    distance
    
  out$dist12 <- dist12[
    1:(dim(out$dist11)[1]),
    (dim(out$dist11)[1] + 1):(dim(dist12)[1])
  ]
  
  return(out)
  
}
data_list <- data_prep(
  data$df
)

gp_mhgibbs_predict <- function(
    seed, num_mcmc, warmup = num_mcmc/2, 
    data_list,
    phi_a = .1, phi_b = .1, sigma_a = .1, sigma_b = .1, delta = .0001, 
    initial_prop_var = .01, adapt = TRUE, adapt_period = .1*num_mcmc, sd = 2.4^2, epsilon = 1e-6){
  # hoff text pg 189 for reassurance on posteriors
  # heikki haario (2001) - An adaptive Metropolis algorithm
  
  # seed
  set.seed(seed)
  
  # convenience
  n <- dim(data_list$dist11)[1]
  m <- dim(data_list$dist22)[1]
  dist11 <- data_list$dist11
  dist22 <- data_list$dist22
  dist12 <- data_list$dist12
  z <- data_list$sp
  
  # storage
  sigma_mcmc <- matrix(NA, num_mcmc, 1)
  phi_mcmc <- matrix(NA, num_mcmc, 1)
  accept_ratio <- matrix(0, num_mcmc, 1)
  mu_mcmc <- matrix(NA, num_mcmc, 1)
  pred_mcmc <- matrix(NA, num_mcmc, m)
  
  # priors
  # mu0 <- matrix(0, nrow = p + 1, ncol = 1)
  # Sigma0 <- 10 * diag(1, p + 1, p + 1)
  # Sigma0_inv <- solve(Sigma0)
  # prior_prod <- Sigma0_inv %*% mu0

  # initialize
  sigma <- invgamma::rinvgamma(1, sigma_a, sigma_b); sigma_mcmc[1,] <- sigma
  sigma2 <- sigma^2
  phi <- invgamma::rinvgamma(1, phi_a, phi_b); phi_mcmc[1,] <- phi
  phi2 <- phi^2
  Omega <- exp(-dist11^2/(2*phi2)) + diag(delta, n, n)
  Omega_inv <- solve(Omega)
  
  X_obs <- matrix(1, nrow = length(data_list$sp), ncol = 1)
  XtOX_obs <- t(X_obs) %*% Omega_inv %*% X_obs
  XtOz_obs <- t(X_obs) %*% Omega_inv %*% z

  # adaptive
  Ct <- initial_prop_var
  
  # sampler
  message(paste0("Beginning sampling at "), Sys.time())
  pb <- txtProgressBar(min = 0, max = num_mcmc, style = 3, width = 50, char = "=")
  for(iter in 2:num_mcmc){
    # mu
    V_obs <- solve(1/sigma2 * XtOX_obs + 1/100) + diag(delta, 1, 1)
    m_obs <- V_obs %*% (1/sigma2 * XtOz_obs)
    mu <- c(chol(V_obs) %*% rnorm(1) + m_obs)
    
    # sigma
    a <- sigma_a + n/2
    b <- c(sigma_b + .5 * t(z - mu) %*% Omega_inv %*% (z - mu))
    sigma <- sqrt(invgamma::rinvgamma(1, a, b))
    sigma2 <- sigma^2
    
    # phi - metropolis
    ## adaptive
    if(adapt){
      if(iter <= adapt_period){
        Ct <- initial_prop_var
      } else if(iter > adapt_period){
        Ct <- sd * cov(phi_mcmc[1:(iter-1),,drop = F]) + sd * epsilon
      }
    }
    
    ## proposal
    phi_s <- -1
    while(phi_s <= 0) phi_s <- phi + rnorm(1, 0, sqrt(Ct))
    phi2_s <- phi_s^2
    Omega_s <- exp(-dist11^2/(2*phi2_s)) + diag(delta, n, n)

    ## evaluate proposal
    log_post_current <- mvtnorm::dmvnorm(z, rep(mu,n), sigma2 * Omega, log = T) + invgamma::dinvgamma(phi, phi_a, phi_b, log = T)
    log_post_s <- mvtnorm::dmvnorm(z, rep(mu,n), sigma2 * Omega_s, log = T) + invgamma::dinvgamma(phi_s, phi_a, phi_b, log = T)
    log_r <- log_post_s - log_post_current
    if(log(runif(1)) < log_r){
      phi <- phi_s
      phi2 <- phi_s^2
      Omega <- exp(-dist11^2/(2*phi2)) + diag(delta, n, n)
      Omega_inv <- solve(Omega)
      XtOX_obs <- t(X_obs) %*% Omega_inv %*% X_obs
      XtOz_obs <- t(X_obs) %*% Omega_inv %*% z

      accept_ratio[iter,] <- 1
    }
    
    # predict
    Sigma11_inv <- (1/sigma^2) * Omega_inv
    Sigma12 <- sigma^2 * exp(-dist12^2 / (2*phi2))
    Sigma22 <- sigma^2 * exp(-dist22^2 / (2*phi2))
    
    t.Sigma11_inv.Sigma12 <- t(Sigma11_inv %*% Sigma12)
    Sigma2.1 <- Sigma22 - t.Sigma11_inv.Sigma12 %*% Sigma12
    Sigma2.1[lower.tri(Sigma2.1)] = t(Sigma2.1)[lower.tri(Sigma2.1)]
    mu2.1 <- t.Sigma11_inv.Sigma12 %*% (matrix(c(z) - c(mu), ncol = 1))
    eta <- mvtnorm::rmvnorm(1, mu2.1, Sigma2.1 + diag(delta, dim(Sigma2.1)))
    
    # storage
    sigma_mcmc[iter,] <- sigma
    phi_mcmc[iter,] <- phi
    mu_mcmc[iter,] <- mu
    pred_mcmc[iter,] <- c(eta)

    # progress
    setTxtProgressBar(pb, iter)
  }
  close(pb)
  message(paste0("Ending sampling at "), Sys.time())
  
  samples <- cbind(
    sigma_mcmc[(warmup+1):num_mcmc,],
    phi_mcmc[(warmup+1):num_mcmc,],
    mu_mcmc[(warmup+1):num_mcmc,],
    pred_mcmc[(warmup+1):num_mcmc,]
  )
  
  colnames(samples) <- c(
    "sigma", 
    "phi",
    "mu",
    paste0(rep("pred[", m), 1:(m), rep("]", m))
  )
  
  return(
    list(
      samples = samples,
      accept_ratio = accept_ratio
    )
  )
  
}
```

### One simulated data set

```{r, echo = T, eval = F}
# priors for the length-scale
## estimate an IG dist that place .01 probability on either tail
# stan(
#   file = 'stan programs/invgamPars.stan',
#   data = list(l=1.5, u=16), # (min and max obs distances)
#   iter = 1,
#   warmup = 0,
#   chains = 1,
#   algorithm = "Fixed_param"
# )

# fit model
this_cluster <- makeCluster(3)
samples <- parLapply(
  cl = this_cluster,
  X = 1:3,
  fun = gp_mhgibbs_predict,
  num_mcmc = 10000,
  warmup = 5000,
  data_list = data_list,
  phi_a = 4,
  phi_b = 11.2,
  sigma_a = 1,
  sigma_b = 1,
  initial_prop_var = .075^2,
  adapt = TRUE,
  delta = 1e-06
)
stopCluster(this_cluster)
saveRDS(samples, "rds files/gp_predict/gp_predict_onefit.rds")
```

```{r, echo = T}
samples <- readRDS("rds files/gp_predict/gp_predict_onefit.rds")

# summarize
model_summary <- nimble_summary(
  list(
    samples[[1]][[1]], samples[[2]][[1]], samples[[3]][[1]] 
  )
)
model_summary[,c(1, 5, 9:12)] %>% round(., 4)
```

## Synthetic data simulation

```{r, eval = F}
library(parallel)
rm(data, samples, model_summary, data_list)
gp_sim_predict <- function(nsims, n = 100){
  sum_tbl <- list()
  pb <- txtProgressBar(min = 0, max = nsims, style = 3, width = 50, char = "=")
  for(sim in 1:nsims){
    # simulate data
    sim_dat <- sim_gp_predict(
      n = n, 
      sigma = 1, 
      phi = 3, 
      seed = sim,
      delta = 1e-6,
      obs_z_prop = .6
    )
    data_list <- data_prep(
      sim_dat$df
    )
    
    # fit model - parallel
    tmp_sim <- sim
    this_cluster <- makeCluster(3)
    samples <- parLapply(
      cl = this_cluster,
      X = 1:3,
      fun = gp_mhgibbs_predict,
      num_mcmc = 10000,
      warmup = 5000,
      data_list = data_list,
      phi_a = 4,
      phi_b = 11.2,
      sigma_a = 1,
      sigma_b = 1,
      initial_prop_var = .075^2,
      adapt = TRUE,
      delta = 1e-06
    )
    stopCluster(this_cluster)
    
    sum <- nimble_summary(list(samples[[1]]$samples, samples[[2]]$samples, samples[[3]]$samples), warmup = 0)
    sum_tbl[[sim]] <- tibble(
      param = rownames(sum),
      truth = c(1, 3, 0, data_list$data %>% filter(is.na(obs_z)) %>% select(z) %>% unlist %>% unname),
      mean = sum[,1],
      lwr = sum[,5],
      upr = sum[,9],
      rhat = sum[,10],
      ess_bulk = sum[,11],
      ess_tail = sum[,12]
    ) %>%
      mutate(sim = tmp_sim)
    
    setTxtProgressBar(pb, sim)
  }
  close(pb)
  
  return(do.call("bind_rows", sum_tbl))
}
gp_predict_sims <- gp_sim_predict(100, 10^2)
saveRDS(gp_predict_sims, "rds files/gp_predict/gp_predict_sims_1_3.rds")
```

```{r}
gp_predict_sims <- readRDS("rds files/gp_predict/gp_predict_sims_1_3.rds")
gp_predict_sims %>%
  mutate(
    capture = factor(case_when(
      truth >= lwr & truth <= upr ~ 1, 
      TRUE ~ 0
    ))
  ) %>%
  filter(!grepl("pred[[]", param)) %>%
  ggplot() + 
  geom_linerange(
    aes(xmin = lwr, xmax = upr, x = mean, col = capture, y = sim)
  ) +
  geom_vline(
    data = tibble(param = c("mu", "phi", "sigma"), int = c(0, 3, 1)),
    aes(xintercept = int),
    linetype = "dotdash"
  ) +
  facet_wrap(~ param, scales = "free_x") +
  theme_bw()

gp_predict_sims %>%
  mutate(
    capture = factor(case_when(
      truth >= lwr & truth <= upr ~ 1, 
      TRUE ~ 0
    ))
  ) %>%
  mutate(param = factor(param, levels = gtools::mixedsort(unique(.data$param)))) %>%
  filter(grepl("pred[[]", param)) %>%
  ggplot() + 
  geom_linerange(
    aes(xmin = lwr, xmax = upr, x = mean, col = capture, y = sim)
  ) +
  # geom_vline(
  #   data = tibble(param = c("beta[1]", "phi", "sigma"), int = c(0, 3, 1)),
  #   aes(xintercept = int),
  #   linetype = "dotdash"
  # ) +
  facet_wrap(~ param, scales = "free_x", nrow = 8, ncol = 7) +
  theme_bw()
```

# Latent Gaussian process with prediction

```{r}
rm(list = ls()[-which(ls() == "hook_chunk")])
source("helpers.R")
```

In this section, we imagine that there are two separate but related processes at play; a spatial process and an observation process. The spatial process is partially observed and can be either completely non-colocated with the observation process, partially colocated with the observation process, or completely colocated with the observation process. The observation process depends on the spatial process. 

## Sampling model and priors

__Spatial process__

Sampling model:
\[
\begin{split}
z &\sim \mathcal{N}(\mu, \Sigma) \\
\Sigma &= \sigma^2 \Omega = \sigma^2\exp\left(-\frac{d_{ij}^2}{2\phi^2}\right) \\
z^* &\sim \mathcal{N}(\mu_{2|1}, \Sigma_{2|1})
\end{split}
\]
where $d_{ij}$ represents the distance between two sample locations. 

__Observation process__

Here, we assume the variance is known to be 1 for simplicity. In the future, the observation process will be non-normal (likely negative binomial distributed). 

\[
\begin{split}
y_i &\sim N(x'_i \beta + z^*_i\alpha, 1)
\end{split}
\]

Priors:
\[
\begin{split}
\mu &\sim \mathcal{N}() \\
\alpha &\sim \mathcal{N}() \\
\beta &\sim \mathcal{N}(\mu_0, \Sigma_0) \\
\sigma &\sim IG(a_0, b_0) \\
\phi &\sim IG(c_0, d_0)
\end{split}
\]

## Example simulated data - none colocated

Data generating values: $\beta = 0$, $\sigma = 1$, $\phi = 3$.

```{r}
sim_latentgp_predict <- function(n = 100, seed = 1, sigma = 1, phi = 1, mu = 0, X = matrix(1, nrow = n), beta = 0, alpha = 1, delta = 1e-9, obs_y_prop = .6, obs_z_prop = .5, colocate = "none"){
  # useful functions
  distance <- function(x){
    dist.vec <- parallelDist::parDist(x)
    dist <- as.matrix(dist.vec)
    return(dist)
  }

  # housekeeping
  if((sqrt(n) %% 1) != 0) stop("n should be a perfect square")
  set.seed(seed)
  
  # create grid
  sfc <- st_sfc(st_polygon(list(rbind(c(0,0), c(sqrt(n),0), c(sqrt(n),sqrt(n)), c(0,0)))))
  grid <- st_as_sf(st_make_grid(sfc, cellsize = 1, square = TRUE)) %>% as_tibble
  names(grid) <- "geometry"
  rm(sfc)
  
  # spatial random effects
  coords <- grid %>%
    st_as_sf %>%
    st_centroid %>%
    st_coordinates

  dist_mat <- coords %>%
    as.matrix %>%
    distance
  
  Sigma <- sigma^2 * exp(-dist_mat^2 / (2*phi^2)) + diag(delta, dim(dist_mat))
  grid$z <- c(mvtnorm::rmvnorm(1, rep(mu, n), Sigma))
  grid$y <- c(rnorm(n, X %*% beta + grid$z * alpha, 1))
  
  # various degrees of masking depending on colocation
  if(colocate == "none"){
    grid2 <- grid %>%
      mutate(obs_y_ind = rbinom(n, 1, obs_y_prop))
    
    if((obs_z_prop * n) >= (n - (obs_y_prop * n))){
      grid2$obs_z_ind <- ifelse(grid2$obs_y_ind == 1, 0, 1)
    } else{
      grid2$obs_z_ind  <- rep(0, n)
      grid2$obs_z_ind[which(grid2$obs_y_ind == 0)] <- rbinom(sum(grid2$obs_y_ind == 0), 1, obs_z_prop)
    }
  } else if(colocate == "all"){
    grid2 <- grid %>%
      mutate(obs_y_ind = rbinom(n, 1, obs_y_prop))
    
    if((obs_z_prop * n) >= (obs_y_prop * n)){
      grid2$obs_z_ind <- ifelse(grid2$obs_y_ind == 1, 1, 0)
      grid2$obs_z_ind[which(grid2$obs_y_ind == 0)] <- rbinom(sum(grid2$obs_y_ind == 0), 1, (obs_z_prop - obs_y_prop))
    } else{
      grid2$obs_z_ind <- rep(0, n)
      grid2$obs_z_ind[which(grid2$obs_y_ind == 1)] <-  rbinom(sum(grid2$obs_y_ind == 1), 1, obs_z_prop)
    }
    
  } else{
    grid2 <- grid %>%
      mutate(obs_y_ind = rbinom(n, 1, obs_y_prop)) %>%
      mutate(obs_z_ind = rbinom(n, 1, obs_z_prop))
  }
  
  grid2 <- grid2 %>%
    mutate(
      obs_y = ifelse(obs_y_ind == 1, y, NA),
      obs_z = ifelse(obs_z_ind == 1, z, NA)
    )

  out <- list(
    df = grid2,
    params = list(
      phi = phi, sigma = sigma, phi = phi, beta = beta, alpha = alpha
    ),
    dist_mat = dist_mat,
    coords = coords
  )
  
  return(out)
}

data <- sim_latentgp_predict(
  n = 10^2, 
  sigma = 1, 
  phi = 3, 
  seed = 06302022, 
  delta = 1e-9, 
  colocate = "none"
)

p1 <- data$df %>%
  st_as_sf %>%
  ggplot() +
  geom_sf(aes(fill = z)) +
  theme_bw() +
  labs(title = "Population spatial effects")

p2 <- data$df %>%
  st_as_sf %>%
  filter(obs_z_ind == 1) %>%
  ggplot() +
  geom_sf(aes(fill = z)) +
  theme_bw() +
  labs(title = "Observed spatial effects")

p3 <- data$df %>%
  st_as_sf %>%
  ggplot() +
  geom_sf(aes(fill = y)) +
  theme_bw() +
  labs(title = "Population response")

p4 <- data$df %>%
  st_as_sf %>%
  filter(obs_y_ind == 1) %>%
  ggplot() +
  geom_sf(aes(fill = y)) +
  theme_bw() +
  labs(title = "Observed response")

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)
```

## Full conditional posterior distributions

Worked out, need to .tex up.

## Gibbs sampler

```{r, echo = T}
# prep the data for fitting the model
data_prep <- function(dat, resp_ind = "obs_y_ind", resp = "obs_y", sp_ind = "obs_z_ind", sp = "obs_z"){
  # useful functions
  distance <- function(x){
    dist.vec <- parallelDist::parDist(x)
    dist <- as.matrix(dist.vec)
    return(dist)
  }
  
  # storage
  out <- list()
  out$data <- dat
  dat$ndx <- 1:nrow(dat)
  
  # grab response data and spatial data
  out$resp <- unlist(unname(dat[which(dat[,resp_ind] == 1),resp]))
  out$resp_row_num <- unlist(unname(dat[which(dat[,resp_ind] == 1),"ndx"]))
  out$sp <- unlist(unname(dat[which(dat[,sp_ind] == 1),sp]))
  out$sp_row_num <- unlist(unname(dat[which(dat[,sp_ind] == 1),"ndx"]))
  
  out$z_atobsy <- unlist(unname(dat[which(dat[,resp_ind] == 1), sp]))
  
  # observed distance matrix
  out$dist11 <- dat[which(dat[,sp_ind] == 1),] %>%
    st_as_sf %>%
    st_centroid %>%
    st_coordinates %>%
    as.matrix %>%
    distance
  
  # prediction distance matrix
  out$dist22 <- dat[which(dat[,sp_ind] == 0 & dat[,resp_ind] == 1),] %>%
    st_as_sf %>%
    st_centroid %>%
    st_coordinates %>%
    as.matrix %>%
    distance
  
  dist12 <- bind_rows(
    dat[which(dat[,sp_ind] == 1),],
    dat[which(dat[,sp_ind] == 0 & dat[,resp_ind] == 1),]
  ) %>%
    st_as_sf %>%
    st_centroid %>%
    st_coordinates %>%
    as.matrix %>%
    distance
    
  out$dist12 <- dist12[
    1:(dim(out$dist11)[1]),
    (dim(out$dist11)[1] + 1):(dim(dist12)[1])
  ]
  
  return(out)
  
}
data_list <- data_prep(
  data$df
)

latentgp_mhgibbs_predict <- function(
    seed, num_mcmc, warmup = num_mcmc/2, 
    data_list, X_,
    phi_a = .1, phi_b = .1, sigma_a = .1, sigma_b = .1, delta = .0001, 
    initial_prop_var = .01, adapt = TRUE, adapt_period = .1*num_mcmc, sd = 2.4^2, epsilon = 1e-6){
  # hoff text pg 189 for reassurance on posteriors
  # heikki haario (2001) - An adaptive Metropolis algorithm
  
  # seed
  set.seed(seed)
  
  # convenience
  n <- dim(data_list$dist11)[1]
  m <- dim(data_list$dist22)[1]
  p <- ncol(X_)
  dist11 <- data_list$dist11
  dist22 <- data_list$dist22
  dist12 <- data_list$dist12
  z <- data_list$sp
  y <- data_list$resp
  
  # storage
  sigma_mcmc <- matrix(NA, num_mcmc, 1)
  phi_mcmc <- matrix(NA, num_mcmc, 1)
  accept_ratio <- matrix(0, num_mcmc, 1)
  mu_mcmc <- matrix(NA, num_mcmc, 1)
  beta_mcmc <- matrix(NA, num_mcmc, p + 1)
  pred_mcmc <- matrix(NA, num_mcmc, m)
  
  # priors
  mu0 <- matrix(0, nrow = p + 1, ncol = 1)
  Sigma0 <- 10 * diag(1, p + 1, p + 1)
  Sigma0_inv <- solve(Sigma0)
  prior_prod <- Sigma0_inv %*% mu0

  # initialize
  beta <- chol(Sigma0) %*% rnorm(p + 1) + mu0; beta_mcmc[1,] <- beta
  sigma <- invgamma::rinvgamma(1, sigma_a, sigma_b); sigma_mcmc[1,] <- sigma
  sigma2 <- sigma^2
  phi <- invgamma::rinvgamma(1, phi_a, phi_b); phi_mcmc[1,] <- phi
  phi2 <- phi^2
  Omega <- exp(-dist11^2/(2*phi2)) + diag(delta, n, n)
  Omega_inv <- solve(Omega)
  
  X_obs <- matrix(1, nrow = length(data_list$sp), ncol = 1)
  XtOX_obs <- t(X_obs) %*% Omega_inv %*% X_obs
  XtOz_obs <- t(X_obs) %*% Omega_inv %*% z

  # adaptive
  Ct <- initial_prop_var
  
  # sampler
  message(paste0("Beginning sampling at "), Sys.time())
  pb <- txtProgressBar(min = 0, max = num_mcmc, style = 3, width = 50, char = "=")
  for(iter in 2:num_mcmc){
    # mu
    V_obs <- solve(1/sigma2 * XtOX_obs + 1/100) + diag(delta, p, p)
    m_obs <- V_obs %*% (1/sigma2 * XtOz_obs)
    mu <- c(chol(V_obs) %*% rnorm(1) + m_obs)
    
    # sigma
    a <- sigma_a + n/2
    b <- c(sigma_b + .5 * t(z - mu) %*% Omega_inv %*% (z - mu))
    sigma <- sqrt(invgamma::rinvgamma(1, a, b))
    sigma2 <- sigma^2
    
    # phi - metropolis
    ## adaptive
    if(adapt){
      if(iter <= adapt_period){
        Ct <- initial_prop_var
      } else if(iter > adapt_period){
        Ct <- sd * cov(phi_mcmc[1:(iter-1),,drop = F]) + sd * epsilon
      }
    }
    
    ## proposal
    phi_s <- -1
    while(phi_s <= 0) phi_s <- phi + rnorm(1, 0, sqrt(Ct))
    phi2_s <- phi_s^2
    Omega_s <- exp(-dist11^2/(2*phi2_s)) + diag(delta, n, n)

    ## evaluate proposal
    log_post_current <- mvtnorm::dmvnorm(z, rep(mu,n), sigma2 * Omega, log = T) + invgamma::dinvgamma(phi, phi_a, phi_b, log = T)
    log_post_s <- mvtnorm::dmvnorm(z, rep(mu,n), sigma2 * Omega_s, log = T) + invgamma::dinvgamma(phi_s, phi_a, phi_b, log = T)
    log_r <- log_post_s - log_post_current
    if(log(runif(1)) < log_r){
      phi <- phi_s
      phi2 <- phi_s^2
      Omega <- exp(-dist11^2/(2*phi2)) + diag(delta, n, n)
      Omega_inv <- solve(Omega)
      XtOX_obs <- t(X_obs) %*% Omega_inv %*% X_obs
      XtOz_obs <- t(X_obs) %*% Omega_inv %*% z

      accept_ratio[iter,] <- 1
    }
    
    # predict
    Sigma11_inv <- (1/sigma^2) * Omega_inv
    Sigma12 <- sigma^2 * exp(-dist12^2 / (2*phi2))
    Sigma22 <- sigma^2 * exp(-dist22^2 / (2*phi2))
    
    t.Sigma11_inv.Sigma12 <- t(Sigma11_inv %*% Sigma12)
    Sigma2.1 <- Sigma22 - t.Sigma11_inv.Sigma12 %*% Sigma12
    Sigma2.1[lower.tri(Sigma2.1)] = t(Sigma2.1)[lower.tri(Sigma2.1)]
    mu2.1 <- t.Sigma11_inv.Sigma12 %*% (matrix(c(z) - c(mu), ncol = 1))
    eta <- mvtnorm::rmvnorm(1, mu2.1, Sigma2.1 + diag(delta, dim(Sigma2.1)))

    # betas
    pred <- data_list$z_atobsy
    pred[which(is.na(pred))] <- c(eta)
    model_mat <- cbind(
      X_, 
      pred
    )
    
    ## assume variance is known and 1
    V_beta <- solve(t(model_mat) %*% model_mat + Sigma0_inv)
    m_beta <- V_beta %*% (t(model_mat) %*% y + prior_prod)
    beta <- chol(V_beta + diag(delta, dim(V_beta))) %*% rnorm(p+1) + m_beta
    
    # storage
    beta_mcmc[iter,] <- c(beta)
    sigma_mcmc[iter,] <- sigma
    phi_mcmc[iter,] <- phi
    mu_mcmc[iter,] <- mu
    pred_mcmc[iter,] <- c(eta)

    # progress
    setTxtProgressBar(pb, iter)
  }
  close(pb)
  message(paste0("Ending sampling at "), Sys.time())
  
  samples <- cbind(
    beta_mcmc[(warmup+1):num_mcmc,],
    sigma_mcmc[(warmup+1):num_mcmc,],
    phi_mcmc[(warmup+1):num_mcmc,],
    mu_mcmc[(warmup+1):num_mcmc,],
    pred_mcmc[(warmup+1):num_mcmc,]
  )
  
  colnames(samples) <- c(
    paste0(rep("beta[", p), 1:(p), rep("]", p)),
    "alpha[1]",
    "sigma", 
    "phi",
    "mu",
    paste0(rep("pred[", m), 1:(m), rep("]", m))
  )
  
  return(
    list(
      samples = samples,
      accept_ratio = accept_ratio
    )
  )
  
}
```

### One simulated data set

```{r, echo = T, eval = F}
# priors for the length-scale
## estimate an IG dist that place .01 probability on either tail
# stan(
#   file = 'stan programs/invgamPars.stan',
#   data = list(l=1.5, u=16), # (min and max obs distances)
#   iter = 1,
#   warmup = 0,
#   chains = 1,
#   algorithm = "Fixed_param"
# )

# fit model
this_cluster <- makeCluster(3)
samples <- parLapply(
  cl = this_cluster,
  X = 1:3,
  fun = latentgp_mhgibbs_predict,
  num_mcmc = 10000,
  warmup = 5000,
  data_list = data_list,
  X_ = cbind(
    rep(1, length(data_list$resp))
  ),
  phi_a = 4,
  phi_b = 11.2,
  sigma_a = 1,
  sigma_b = 1,
  initial_prop_var = .075^2,
  adapt = TRUE,
  delta = 1e-06
)
stopCluster(this_cluster)
saveRDS(samples, "rds files/latentgp_predict/latentgp_predict_onefit.rds")
```

```{r, echo = T}
samples <- readRDS("rds files/latentgp_predict/latentgp_predict_onefit.rds")

# summarize
model_summary <- nimble_summary(
  list(
    samples[[1]][[1]], samples[[2]][[1]], samples[[3]][[1]] 
  )
)
model_summary[,c(1, 5, 9:12)] %>% round(., 4)
```

## Synthetic data simulation

```{r, eval = F}
library(parallel)
latentgp_sim_predict <- function(nsims, n = 100){
  sum_tbl <- list()
  pb <- txtProgressBar(min = 0, max = nsims, style = 3, width = 50, char = "=")
  for(sim in 1:nsims){
    # simulate data
    sim_dat <- sim_latentgp_predict(
      n = 10^2, 
      sigma = 1, 
      phi = 3, 
      seed = sim, 
      delta = 1e-6, 
      colocate = "none"
    )
    data_list <- data_prep(
      sim_dat$df
    )
    
    # fit model - parallel
    tmp_sim <- sim
    this_cluster <- makeCluster(3)
    samples <- parLapply(
      cl = this_cluster,
      X = 1:3,
      fun = latentgp_mhgibbs_predict,
      num_mcmc = 10000,
      warmup = 5000,
      data_list = data_list,
      X_ = cbind(
        rep(1, length(data_list$resp))
      ),
      phi_a = 4,
      phi_b = 11.2,
      sigma_a = 1,
      sigma_b = 1,
      initial_prop_var = .075^2,
      adapt = TRUE,
      delta = 1e-06
    )
    stopCluster(this_cluster)
    
    sum <- nimble_summary(
      list(samples[[1]]$samples, samples[[2]]$samples, samples[[3]]$samples), 
      warmup = 0
    )
    sum_tbl[[sim]] <- tibble(
      param = rownames(sum),
      truth = c(
        0, 1, 1, 3, 0,
        data_list$data %>% filter(is.na(obs_z)) %>% select(z) %>% unlist %>% unname
      ),
      mean = sum[,1],
      lwr = sum[,5],
      upr = sum[,9],
      rhat = sum[,10],
      ess_bulk = sum[,11],
      ess_tail = sum[,12]
    ) %>%
      mutate(sim = tmp_sim)
    
    setTxtProgressBar(pb, sim)
  }
  close(pb)
  
  return(do.call("bind_rows", sum_tbl))
}
latentgp_sims_predict <- latentgp_sim_predict(100, 100)
saveRDS(latentgp_sims_predict, "rds files/latentgp/latentgp_sims_predict_1_3.rds")
```

```{r}
latentgp_predict_sims <- readRDS("rds files/latentgp_predict/latentgp_sims_predict_1_3.rds")
latentgp_predict_sims %>%
  mutate(
    capture = factor(case_when(
      truth >= lwr & truth <= upr ~ 1, 
      TRUE ~ 0
    ))
  ) %>%
  mutate(param = ifelse(param == "beta[2]", "alpha[1]", param)) %>%
  filter(!grepl("pred[[]", param)) %>%
  ggplot() + 
  geom_linerange(
    aes(xmin = lwr, xmax = upr, x = mean, col = capture, y = sim)
  ) +
  geom_vline(
    data = tibble(param = c("alpha[1]", "beta[1]", "mu", "phi", "sigma"), int = c(1, 0, 0, 3, 1)),
    aes(xintercept = int),
    linetype = "dotdash"
  ) +
  facet_wrap(~ param, scales = "free_x") +
  theme_bw()

latentgp_predict_sims %>%
  mutate(
    capture = factor(case_when(
      truth >= lwr & truth <= upr ~ 1, 
      TRUE ~ 0
    ))
  ) %>%
  mutate(param = factor(param, levels = gtools::mixedsort(unique(.data$param)))) %>%
  filter(grepl("pred[[]", param)) %>%
  ggplot() + 
  geom_linerange(
    aes(xmin = lwr, xmax = upr, x = mean, col = capture, y = sim)
  ) +
  # geom_vline(
  #   data = tibble(param = c("beta[1]", "phi", "sigma"), int = c(0, 3, 1)),
  #   aes(xintercept = int),
  #   linetype = "dotdash"
  # ) +
  facet_wrap(~ param, scales = "free_x") +
  theme_bw()
```
